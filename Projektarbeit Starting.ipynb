{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5678e000",
   "metadata": {},
   "source": [
    "reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1696ba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      " NYC Taxi Trip Data Cleansing - February 2025\n",
      "==============================================================\n",
      "\n",
      "Initial rows loaded: 3,577,543\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# NYC Yellow Taxi Trip Data Cleansing - February 2025\n",
    "# Enhanced Fare Validation\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1. Setup\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"yellow_tripdata_2025-02.parquet\")\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "print(\"==============================================================\")\n",
    "print(\" NYC Taxi Trip Data Cleansing - February 2025\")\n",
    "print(\"==============================================================\\n\")\n",
    "\n",
    "initial_rows = len(df)\n",
    "print(f\"Initial rows loaded: {initial_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79278af",
   "metadata": {},
   "source": [
    "CleaningReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6453c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_report = []\n",
    "\n",
    "def log_step(step_name, before, after):\n",
    "    cleaning_report.append({\n",
    "        \"Step\": step_name,\n",
    "        \"Rows Before\": before,\n",
    "        \"Rows After\": after,\n",
    "        \"Rows Dropped\": before - after,\n",
    "        \"Remaining %\": round(after / initial_rows * 100, 2)\n",
    "    })\n",
    "    print(f\"[{step_name}] - Dropped {before - after:,} rows \"\n",
    "          f\"({round((before - after)/before*100,2) if before>0 else 0:.2f}%)\")\n",
    "\n",
    "def validate_column(df, condition, col_name, description=\"\"):\n",
    "    invalid_count = (~condition).sum()\n",
    "    print(f\"{col_name}: {invalid_count:,} invalid {description}\")\n",
    "    return df[condition].copy(), invalid_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24ad94",
   "metadata": {},
   "source": [
    "Output of general informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9bef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Information about the DataFrame:\n",
      "First 5 rows:\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2025-02-01 00:12:18   2025-02-01 00:32:33              3.0   \n",
      "1         2  2025-02-01 00:40:04   2025-02-01 00:49:15              1.0   \n",
      "2         1  2025-02-01 00:06:09   2025-02-01 00:11:51              0.0   \n",
      "3         1  2025-02-01 00:15:13   2025-02-01 00:20:19              0.0   \n",
      "4         2  2025-02-01 00:02:52   2025-02-01 00:20:25              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           3.12         1.0                  N           246            79   \n",
      "1           1.40         1.0                  N           114            79   \n",
      "2           0.40         1.0                  N           211           144   \n",
      "3           0.70         1.0                  N           113           249   \n",
      "4           4.19         1.0                  N           113           263   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             1         19.8   1.00      0.5        5.11           0.0   \n",
      "1             1         10.0   1.00      0.5        3.15           0.0   \n",
      "2             1          6.5   4.25      0.5        1.00           0.0   \n",
      "3             1          7.2   4.25      0.5        2.00           0.0   \n",
      "4             1         19.8   1.00      0.5        5.11           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \\\n",
      "0                    1.0         30.66                   2.5          0.0   \n",
      "1                    1.0         18.90                   2.5          0.0   \n",
      "2                    1.0         13.25                   2.5          0.0   \n",
      "3                    1.0         14.95                   2.5          0.0   \n",
      "4                    1.0         30.66                   2.5          0.0   \n",
      "\n",
      "   cbd_congestion_fee  \n",
      "0                0.75  \n",
      "1                0.75  \n",
      "2                0.75  \n",
      "3                0.75  \n",
      "4                0.75  \n",
      "*******************************************\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3577543 entries, 0 to 3577542\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int32         \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   store_and_fwd_flag     object        \n",
      " 7   PULocationID           int32         \n",
      " 8   DOLocationID           int32         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      " 18  Airport_fee            float64       \n",
      " 19  cbd_congestion_fee     float64       \n",
      "dtypes: datetime64[us](2), float64(13), int32(3), int64(1), object(1)\n",
      "memory usage: 504.9+ MB\n",
      "None\n",
      "*******************************************\n",
      "Count:\n",
      "VendorID                 3577543\n",
      "tpep_pickup_datetime     3577543\n",
      "tpep_dropoff_datetime    3577543\n",
      "passenger_count          2770606\n",
      "trip_distance            3577543\n",
      "RatecodeID               2770606\n",
      "store_and_fwd_flag       2770606\n",
      "PULocationID             3577543\n",
      "DOLocationID             3577543\n",
      "payment_type             3577543\n",
      "fare_amount              3577543\n",
      "extra                    3577543\n",
      "mta_tax                  3577543\n",
      "tip_amount               3577543\n",
      "tolls_amount             3577543\n",
      "improvement_surcharge    3577543\n",
      "total_amount             3577543\n",
      "congestion_surcharge     2770606\n",
      "Airport_fee              2770606\n",
      "cbd_congestion_fee       3577543\n",
      "dtype: int64\n",
      "*******************************************\n",
      "Descriptive Statistics:\n",
      "           VendorID        tpep_pickup_datetime       tpep_dropoff_datetime  \\\n",
      "count  3.577543e+06                     3577543                     3577543   \n",
      "mean   1.795510e+00  2025-02-15 03:32:25.140901  2025-02-15 03:47:49.613170   \n",
      "min    1.000000e+00         2025-01-31 22:22:53         2025-01-31 22:30:00   \n",
      "25%    2.000000e+00         2025-02-08 01:18:52         2025-02-08 01:31:19   \n",
      "50%    2.000000e+00         2025-02-14 21:06:49         2025-02-14 21:23:14   \n",
      "75%    2.000000e+00  2025-02-21 23:07:40.500000  2025-02-21 23:21:48.500000   \n",
      "max    7.000000e+00         2025-03-01 00:06:32         2025-03-01 23:13:42   \n",
      "std    4.489799e-01                         NaN                         NaN   \n",
      "\n",
      "       passenger_count  trip_distance    RatecodeID  PULocationID  \\\n",
      "count     2.770606e+06   3.577543e+06  2.770606e+06  3.577543e+06   \n",
      "mean      1.276028e+00   6.025357e+00  2.458886e+00  1.632846e+02   \n",
      "min       0.000000e+00   0.000000e+00  1.000000e+00  1.000000e+00   \n",
      "25%       1.000000e+00   1.000000e+00  1.000000e+00  1.250000e+02   \n",
      "50%       1.000000e+00   1.700000e+00  1.000000e+00  1.610000e+02   \n",
      "75%       1.000000e+00   3.180000e+00  1.000000e+00  2.330000e+02   \n",
      "max       9.000000e+00   2.287825e+05  9.900000e+01  2.650000e+02   \n",
      "std       7.223483e-01   5.433970e+02  1.156332e+01  6.562958e+01   \n",
      "\n",
      "       DOLocationID  payment_type   fare_amount         extra       mta_tax  \\\n",
      "count  3.577543e+06  3.577543e+06  3.577543e+06  3.577543e+06  3.577543e+06   \n",
      "mean   1.624813e+02  9.428504e-01  1.674901e+01  1.238108e+00  4.805158e-01   \n",
      "min    1.000000e+00  0.000000e+00 -1.807600e+03 -7.500000e+00 -5.000000e-01   \n",
      "25%    1.130000e+02  1.000000e+00  8.600000e+00  0.000000e+00  5.000000e-01   \n",
      "50%    1.620000e+02  1.000000e+00  1.280000e+01  0.000000e+00  5.000000e-01   \n",
      "75%    2.340000e+02  1.000000e+00  1.980000e+01  2.500000e+00  5.000000e-01   \n",
      "max    2.650000e+02  4.000000e+00  1.325314e+05  2.255000e+01  1.050000e+01   \n",
      "std    6.986419e+01  7.249215e-01  7.212927e+01  1.842253e+00  1.299273e-01   \n",
      "\n",
      "         tip_amount  tolls_amount  improvement_surcharge  total_amount  \\\n",
      "count  3.577543e+06  3.577543e+06           3.577543e+06  3.577543e+06   \n",
      "mean   2.730064e+00  4.052944e-01           9.591618e-01  2.503055e+01   \n",
      "min   -2.200000e+02 -1.139300e+02          -1.000000e+00 -1.832850e+03   \n",
      "25%    0.000000e+00  0.000000e+00           1.000000e+00  1.531000e+01   \n",
      "50%    2.110000e+00  0.000000e+00           1.000000e+00  2.019000e+01   \n",
      "75%    3.790000e+00  0.000000e+00           1.000000e+00  2.797000e+01   \n",
      "max    4.400000e+02  1.158700e+02           1.000000e+00  1.325554e+05   \n",
      "std    3.652688e+00  1.889599e+00           2.643990e-01  7.318542e+01   \n",
      "\n",
      "       congestion_surcharge   Airport_fee  cbd_congestion_fee  \n",
      "count          2.770606e+06  2.770606e+06        3.577543e+06  \n",
      "mean           2.231689e+00  1.216311e-01        5.373177e-01  \n",
      "min           -2.500000e+00 -1.750000e+00       -7.500000e-01  \n",
      "25%            2.500000e+00  0.000000e+00        0.000000e+00  \n",
      "50%            2.500000e+00  0.000000e+00        7.500000e-01  \n",
      "75%            2.500000e+00  0.000000e+00        7.500000e-01  \n",
      "max            2.500000e+00  6.750000e+00        1.250000e+00  \n",
      "std            8.951699e-01  4.691379e-01        3.549505e-01  \n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataframe\n",
    "# print first 5 rows, info, count, and descriptive statistics\n",
    "print(\"Basic Information about the DataFrame:\")\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head(5))\n",
    "print(\"*******************************************\")\n",
    "print(\"Info:\")\n",
    "print(df.info())\n",
    "print(\"*******************************************\")\n",
    "print(\"Count:\")\n",
    "print(df.count())\n",
    "print(\"*******************************************\")\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(df.describe())\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7758b",
   "metadata": {},
   "source": [
    "STARTING CLEANSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e78a197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Number of rows in the dataframe: 3,577,543\n",
      "===== STARTING CLEANSING =====\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Initial number of rows in the dataframe\n",
    "initial_row_count = len(df)\n",
    "print(f\"Initial Number of rows in the dataframe: {initial_row_count:,.0f}\")\n",
    "# print STARTING CLEANSING\n",
    "print(\"===== STARTING CLEANSING =====\")\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c33cc",
   "metadata": {},
   "source": [
    "VendorID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c17270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No invalid VendorID entries found.\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Column VendorID allowed values\n",
    "allowed_vendor_ids = [1, 2, 6, 7]\n",
    "\n",
    "# Create boolean mask for invalid VendorIDs\n",
    "invalid_vendor = ~df['VendorID'].isin(allowed_vendor_ids)\n",
    "invalid_count = invalid_vendor.sum()\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(\"=== VendorID Validation Report ===\")\n",
    "    print(f\"Rows before validation: {len(df):,}\")\n",
    "    print(f\"Invalid VendorID entries: {invalid_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Show which invalid VendorIDs were found\n",
    "    invalid_values = df.loc[invalid_vendor, 'VendorID'].value_counts().sort_index()\n",
    "    print(f\"\\nInvalid VendorID values found:\")\n",
    "    for vendor_id, count in invalid_values.items():\n",
    "        print(f\"  VendorID {vendor_id}: {count:,} rows\")\n",
    "    \n",
    "    # Drop invalid VendorIDs\n",
    "    df = df[~invalid_vendor].copy()\n",
    "    print(f\"\\nDropped invalid VendorID entries.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "else:\n",
    "    print(\"No invalid VendorID entries found.\")\n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bfae4a",
   "metadata": {},
   "source": [
    "Dates (tpep_pickup_datetime & tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9f1055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5,164 rows with incorrect order - swapping pickup/dropoff times\n",
      "\n",
      "=== Datetime Validation Report ===\n",
      "Rows before validation: 3,577,543\n",
      "Invalid tpep_pickup_datetime rows: 0\n",
      "Invalid tpep_dropoff_datetime rows: 0\n",
      "Rows with swapped pickup/dropoff: 5,164\n",
      "Rows where dropoff not in February 2025: 2,406\n",
      "\n",
      "Total rows dropped: 2,406\n",
      "Percentage dropped: 0.07%\n",
      "\n",
      "Rows remaining: 3,575,137\n",
      "*******************************************\n",
      "\n",
      "=== Trip Duration Statistics (Before Filtering) ===\n",
      "Mean duration: 15.39 minutes\n",
      "Median duration: 12.20 minutes\n",
      "Min duration: 0.00 minutes\n",
      "Max duration: 8564.67 minutes\n",
      "Std deviation: 26.19 minutes\n",
      "\n",
      "Trips with duration < 1 minute: 40,229 (1.13%)\n",
      "Trips with duration > 5 hours: 1,132 (0.03%)\n",
      "\n",
      "=== Extreme Duration Validation ===\n",
      "Total trips with extreme duration (< 1 min or > 5 hours): 41,361\n",
      "Percentage: 1.16%\n",
      "Dropped extreme duration trips.\n",
      "Rows remaining: 3,533,776\n",
      "\n",
      "=== Trip Duration Statistics (After Filtering) ===\n",
      "Mean duration: 15.17 minutes\n",
      "Median duration: 12.32 minutes\n",
      "Min duration: 1.00 minutes\n",
      "Max duration: 299.47 minutes\n",
      "Std deviation: 11.45 minutes\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Definition: tpep_dropoff_datetime determines the month of the trip/payment, should be February 2025!\n",
    "\n",
    "# Convert datetime columns once (efficient approach)\n",
    "pickup_dt = pd.to_datetime(df['tpep_pickup_datetime'], errors='coerce')\n",
    "dropoff_dt = pd.to_datetime(df['tpep_dropoff_datetime'], errors='coerce')\n",
    "\n",
    "# Create boolean masks for each validation check\n",
    "invalid_pickup = pickup_dt.isna()\n",
    "invalid_dropoff = dropoff_dt.isna()\n",
    "\n",
    "# Check for rows where order needs to be swapped\n",
    "valid_both = ~invalid_pickup & ~invalid_dropoff\n",
    "needs_swap = valid_both & (dropoff_dt <= pickup_dt)\n",
    "swap_count = needs_swap.sum()\n",
    "\n",
    "# Swap pickup and dropoff where needed\n",
    "if swap_count > 0:\n",
    "    print(f\"Found {swap_count:,} rows with incorrect order - swapping pickup/dropoff times\")\n",
    "    pickup_temp = pickup_dt.copy()\n",
    "    pickup_dt = pickup_dt.where(~needs_swap, dropoff_dt)\n",
    "    dropoff_dt = dropoff_dt.where(~needs_swap, pickup_temp)\n",
    "\n",
    "# Now check date range on corrected dropoff times\n",
    "invalid_month = valid_both & ((dropoff_dt.dt.month != 2) | (dropoff_dt.dt.year != 2025))\n",
    "\n",
    "# Combine invalid conditions (excluding order issues since we fixed them)\n",
    "all_invalid = invalid_pickup | invalid_dropoff | invalid_month\n",
    "invalid_count = all_invalid.sum()\n",
    "\n",
    "if invalid_count > 0 or swap_count > 0:\n",
    "    print(\"\\n=== Datetime Validation Report ===\")\n",
    "    print(f\"Rows before validation: {len(df):,}\")\n",
    "    print(f\"Invalid tpep_pickup_datetime rows: {invalid_pickup.sum():,}\")\n",
    "    print(f\"Invalid tpep_dropoff_datetime rows: {invalid_dropoff.sum():,}\")\n",
    "    print(f\"Rows with swapped pickup/dropoff: {swap_count:,}\")\n",
    "    print(f\"Rows where dropoff not in February 2025: {invalid_month.sum():,}\")\n",
    "    print(f\"\\nTotal rows dropped: {invalid_count:,}\")\n",
    "    print(f\"Percentage dropped: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Drop invalid rows and update datetime columns\n",
    "    df = df[~all_invalid].copy()\n",
    "    df['tpep_pickup_datetime'] = pickup_dt[~all_invalid]\n",
    "    df['tpep_dropoff_datetime'] = dropoff_dt[~all_invalid]\n",
    "    \n",
    "    print(f\"\\nRows remaining: {len(df):,}\")\n",
    "else:\n",
    "    print(\"No invalid datetime entries found.\")\n",
    "    # Still convert to datetime dtype for consistency\n",
    "    df['tpep_pickup_datetime'] = pickup_dt\n",
    "    df['tpep_dropoff_datetime'] = dropoff_dt\n",
    "\n",
    "print(\"*******************************************\")\n",
    "\n",
    "# Calculate trip duration in minutes\n",
    "df['trip_duration_minutes'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Show duration statistics BEFORE filtering\n",
    "print(\"\\n=== Trip Duration Statistics (Before Filtering) ===\")\n",
    "print(f\"Mean duration: {df['trip_duration_minutes'].mean():.2f} minutes\")\n",
    "print(f\"Median duration: {df['trip_duration_minutes'].median():.2f} minutes\")\n",
    "print(f\"Min duration: {df['trip_duration_minutes'].min():.2f} minutes\")\n",
    "print(f\"Max duration: {df['trip_duration_minutes'].max():.2f} minutes\")\n",
    "print(f\"Std deviation: {df['trip_duration_minutes'].std():.2f} minutes\")\n",
    "\n",
    "# Count trips with extreme durations\n",
    "short_trips_count = (df['trip_duration_minutes'] < 1).sum()\n",
    "long_trips_count = (df['trip_duration_minutes'] > 300).sum()\n",
    "\n",
    "print(f\"\\nTrips with duration < 1 minute: {short_trips_count:,} ({(short_trips_count / len(df)) * 100:.2f}%)\")\n",
    "print(f\"Trips with duration > 5 hours: {long_trips_count:,} ({(long_trips_count / len(df)) * 100:.2f}%)\")\n",
    "\n",
    "# Filter out extreme durations\n",
    "extreme_duration = (df['trip_duration_minutes'] < 1) | (df['trip_duration_minutes'] > 300)\n",
    "extreme_count = extreme_duration.sum()\n",
    "\n",
    "if extreme_count > 0:\n",
    "    print(f\"\\n=== Extreme Duration Validation ===\")\n",
    "    print(f\"Total trips with extreme duration (< 1 min or > 5 hours): {extreme_count:,}\")\n",
    "    print(f\"Percentage: {(extreme_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Drop extreme duration trips\n",
    "    df = df[~extreme_duration].copy()\n",
    "    \n",
    "    print(f\"Dropped extreme duration trips.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "    \n",
    "    # Show updated statistics AFTER filtering\n",
    "    print(f\"\\n=== Trip Duration Statistics (After Filtering) ===\")\n",
    "    print(f\"Mean duration: {df['trip_duration_minutes'].mean():.2f} minutes\")\n",
    "    print(f\"Median duration: {df['trip_duration_minutes'].median():.2f} minutes\")\n",
    "    print(f\"Min duration: {df['trip_duration_minutes'].min():.2f} minutes\")\n",
    "    print(f\"Max duration: {df['trip_duration_minutes'].max():.2f} minutes\")\n",
    "    print(f\"Std deviation: {df['trip_duration_minutes'].std():.2f} minutes\")\n",
    "else:\n",
    "    print(\"\\nNo extreme duration trips found.\")\n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767c4c3",
   "metadata": {},
   "source": [
    "passenger_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a4c3206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Passenger Count Validation Report ===\n",
      "Rows before validation: 3,533,776\n",
      "NaN passenger_count rows: 803,554\n",
      "Passenger_count <= 0 rows: 21,060\n",
      "Passenger_count > 10 rows: 0\n",
      "\n",
      "Total invalid passenger_count entries: 824,614\n",
      "Percentage of invalid entries: 23.05%\n",
      "\n",
      "Set invalid passenger_count values to default of 1.\n",
      "Rows remaining: 3,533,776\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Invalid passenger_count: NaN or <= 0 or > 10\n",
    "# Create boolean masks for each validation check\n",
    "is_nan = df['passenger_count'].isna()\n",
    "is_zero_or_negative = df['passenger_count'] <= 0\n",
    "is_too_high = df['passenger_count'] > 10\n",
    "\n",
    "# Combine all invalid conditions\n",
    "all_invalid = is_nan | is_zero_or_negative | is_too_high\n",
    "invalid_count = all_invalid.sum()\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(\"=== Passenger Count Validation Report ===\")\n",
    "    print(f\"Rows before validation: {len(df):,}\")\n",
    "    print(f\"NaN passenger_count rows: {is_nan.sum():,}\")\n",
    "    print(f\"Passenger_count <= 0 rows: {is_zero_or_negative.sum():,}\")\n",
    "    print(f\"Passenger_count > 10 rows: {is_too_high.sum():,}\")\n",
    "    print(f\"\\nTotal invalid passenger_count entries: {invalid_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Set invalid passenger_count values to default of 1\n",
    "    df.loc[all_invalid, 'passenger_count'] = 1\n",
    "    \n",
    "    print(f\"\\nSet invalid passenger_count values to default of 1.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "else:\n",
    "    print(\"No invalid passenger_count entries found.\")\n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea60f28",
   "metadata": {},
   "source": [
    "trip_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fdb4b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Trip Distance Validation Report ===\n",
      "Rows before validation: 3,533,776\n",
      "Negative trip_distance rows: 0\n",
      "Zero trip_distance rows: 75,521\n",
      "NaN trip_distance rows: 0\n",
      "Trip_distance > 500 rows: 141\n",
      "\n",
      "Total invalid trip_distance entries: 75,662\n",
      "Percentage of invalid entries: 2.11%\n",
      "\n",
      "Dropped invalid trip_distance entries.\n",
      "Rows remaining: 3,458,114\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Invalid trip_distance: NaN or <= 0 or > 500\n",
    "# Create boolean masks for each validation check\n",
    "is_nan = df['trip_distance'].isna()\n",
    "is_negative = df['trip_distance'] < 0\n",
    "is_zero = df['trip_distance'] == 0\n",
    "is_too_high = df['trip_distance'] > 500\n",
    "\n",
    "# Combine all invalid conditions\n",
    "all_invalid = is_nan | is_negative | is_zero | is_too_high\n",
    "invalid_count = all_invalid.sum()\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(\"=== Trip Distance Validation Report ===\")\n",
    "    print(f\"Rows before validation: {len(df):,}\")\n",
    "    print(f\"Negative trip_distance rows: {is_negative.sum():,}\")\n",
    "    print(f\"Zero trip_distance rows: {is_zero.sum():,}\")\n",
    "    print(f\"NaN trip_distance rows: {is_nan.sum():,}\")\n",
    "    print(f\"Trip_distance > 500 rows: {is_too_high.sum():,}\")\n",
    "    print(f\"\\nTotal invalid trip_distance entries: {invalid_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Drop invalid trip_distance entries\n",
    "    df = df[~all_invalid].copy()\n",
    "    \n",
    "    print(f\"\\nDropped invalid trip_distance entries.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "else:\n",
    "    print(\"No invalid trip_distance entries found.\")\n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e68c29",
   "metadata": {},
   "source": [
    "RatecodeID\n",
    "correct rate code might be possible to be derifed from other data, but for simplicity we set to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d52ce780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RatecodeID Missing Values ===\n",
      "Rows before validation: 3,458,114\n",
      "NaN/Missing RatecodeID: 741,846 rows (21.45%)\n",
      "Set missing RatecodeID values to default of 1 (Standard rate).\n",
      "\n",
      "=== RatecodeID Overview ===\n",
      "Total rows: 3,458,114\n",
      "\n",
      "Valid RatecodeID distribution:\n",
      "  RatecodeID 1.0: 3,319,390 rows (95.99%)\n",
      "  RatecodeID 2.0: 77,739 rows (2.25%)\n",
      "  RatecodeID 3.0: 7,119 rows (0.21%)\n",
      "  RatecodeID 4.0: 6,331 rows (0.18%)\n",
      "  RatecodeID 5.0: 10,864 rows (0.31%)\n",
      "  RatecodeID 99.0: 36,671 rows (1.06%)\n",
      "\n",
      "No invalid RatecodeID entries found.\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Column RatecodeID allowed values\n",
    "allowed_ratecode_ids = [1, 2, 3, 4, 5, 6, 99]\n",
    "\n",
    "# Check for NaN values first and set to default\n",
    "nan_count = df['RatecodeID'].isna().sum()\n",
    "if nan_count > 0:\n",
    "    print(\"=== RatecodeID Missing Values ===\")\n",
    "    print(f\"Rows before validation: {len(df):,}\")\n",
    "    print(f\"NaN/Missing RatecodeID: {nan_count:,} rows ({(nan_count / len(df)) * 100:.2f}%)\")\n",
    "    df['RatecodeID'] = df['RatecodeID'].fillna(1)\n",
    "    print(f\"Set missing RatecodeID values to default of 1 (Standard rate).\\n\")\n",
    "    \n",
    "# Create boolean mask for invalid RatecodeIDs (after filling NaN)\n",
    "invalid_ratecode = ~df['RatecodeID'].isin(allowed_ratecode_ids)\n",
    "invalid_count = invalid_ratecode.sum()\n",
    "\n",
    "# Show overview of all RatecodeID values\n",
    "print(\"=== RatecodeID Overview ===\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"\\nValid RatecodeID distribution:\")\n",
    "valid_values = df.loc[~invalid_ratecode, 'RatecodeID'].value_counts().sort_index()\n",
    "for ratecode_id, count in valid_values.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  RatecodeID {ratecode_id}: {count:,} rows ({percentage:.2f}%)\")\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(f\"\\n=== Invalid RatecodeID Values ===\")\n",
    "    print(f\"Invalid RatecodeID entries: {invalid_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Show which invalid RatecodeIDs were found\n",
    "    invalid_values = df.loc[invalid_ratecode, 'RatecodeID'].value_counts().sort_index()\n",
    "    print(f\"\\nInvalid RatecodeID values found:\")\n",
    "    for ratecode_id, count in invalid_values.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  RatecodeID {ratecode_id}: {count:,} rows ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Drop invalid RatecodeIDs\n",
    "    df = df[~invalid_ratecode].copy()\n",
    "    \n",
    "    print(f\"\\nDropped invalid RatecodeID entries.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "else:\n",
    "    print(\"\\nNo invalid RatecodeID entries found.\")\n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6662b",
   "metadata": {},
   "source": [
    "store_and_fwd_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1cfb3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Store and Forward Flag Missing Values ===\n",
      "Rows before validation: 3,458,114\n",
      "NaN/Missing store_and_fwd_flag: 741,846 rows (21.45%)\n",
      "Set missing store_and_fwd_flag values to default of 'N'.\n",
      "\n",
      "=== Store and Forward Flag Overview ===\n",
      "Total rows: 3,458,114\n",
      "\n",
      "Valid store_and_fwd_flag distribution:\n",
      "  Flag 'N': 3,430,730 rows (99.21%)\n",
      "  Flag 'Y': 27,384 rows (0.79%)\n",
      "\n",
      "No invalid store_and_fwd_flag entries found.\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Column store_and_fwd_flag allowed values\n",
    "allowed_store_and_fwd_flags = ['Y', 'N']\n",
    "\n",
    "# Check for NaN values first and set to default\n",
    "nan_count = df['store_and_fwd_flag'].isna().sum()\n",
    "if nan_count > 0:\n",
    "    print(\"=== Store and Forward Flag Missing Values ===\")\n",
    "    print(f\"Rows before validation: {len(df):,}\")\n",
    "    print(f\"NaN/Missing store_and_fwd_flag: {nan_count:,} rows ({(nan_count / len(df)) * 100:.2f}%)\")\n",
    "    df['store_and_fwd_flag'] = df['store_and_fwd_flag'].fillna('N')\n",
    "    print(f\"Set missing store_and_fwd_flag values to default of 'N'.\\n\")\n",
    "\n",
    "# Create boolean mask for invalid store_and_fwd_flag (after filling NaN)\n",
    "invalid_flag = ~df['store_and_fwd_flag'].isin(allowed_store_and_fwd_flags)\n",
    "invalid_count = invalid_flag.sum()\n",
    "\n",
    "# Show overview of all store_and_fwd_flag values\n",
    "print(\"=== Store and Forward Flag Overview ===\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"\\nValid store_and_fwd_flag distribution:\")\n",
    "valid_values = df.loc[~invalid_flag, 'store_and_fwd_flag'].value_counts().sort_index()\n",
    "for flag, count in valid_values.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  Flag '{flag}': {count:,} rows ({percentage:.2f}%)\")\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(f\"\\n=== Invalid Store and Forward Flag Values ===\")\n",
    "    print(f\"Invalid store_and_fwd_flag entries: {invalid_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Show which invalid flags were found\n",
    "    invalid_values = df.loc[invalid_flag, 'store_and_fwd_flag'].value_counts().sort_index()\n",
    "    print(f\"\\nInvalid store_and_fwd_flag values found:\")\n",
    "    for flag, count in invalid_values.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  Flag '{flag}': {count:,} rows ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Drop invalid store_and_fwd_flag entries\n",
    "    df = df[~invalid_flag].copy()\n",
    "    \n",
    "    print(f\"\\nDropped invalid store_and_fwd_flag entries.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "else:\n",
    "    print(\"\\nNo invalid store_and_fwd_flag entries found.\")\n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05578ff",
   "metadata": {},
   "source": [
    "LocationID (PULocationID & DOLocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1532c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PULocationID (Pickup Location) Validation ===\n",
      "Rows before validation: 3,458,114\n",
      "Top 10 pickup locations:\n",
      "  LocationID 161: 157,397 rows (4.55%)\n",
      "  LocationID 237: 154,285 rows (4.46%)\n",
      "  LocationID 236: 144,699 rows (4.18%)\n",
      "  LocationID 132: 120,772 rows (3.49%)\n",
      "  LocationID 230: 111,855 rows (3.23%)\n",
      "  LocationID 186: 111,167 rows (3.21%)\n",
      "  LocationID 162: 111,070 rows (3.21%)\n",
      "  LocationID 142: 102,272 rows (2.96%)\n",
      "  LocationID 234: 99,776 rows (2.89%)\n",
      "  LocationID 170: 94,134 rows (2.72%)\n",
      "\n",
      "No invalid PULocationID entries found.\n",
      "*******************************************\n",
      "=== DOLocationID (Dropoff Location) Validation ===\n",
      "Top 10 dropoff locations:\n",
      "  LocationID 236: 150,103 rows (4.34%)\n",
      "  LocationID 237: 139,696 rows (4.04%)\n",
      "  LocationID 161: 121,449 rows (3.51%)\n",
      "  LocationID 230: 99,573 rows (2.88%)\n",
      "  LocationID 170: 99,268 rows (2.87%)\n",
      "  LocationID 239: 92,048 rows (2.66%)\n",
      "  LocationID 68: 91,234 rows (2.64%)\n",
      "  LocationID 142: 90,960 rows (2.63%)\n",
      "  LocationID 162: 89,455 rows (2.59%)\n",
      "  LocationID 141: 88,584 rows (2.56%)\n",
      "\n",
      "No invalid DOLocationID entries found.\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# LocationID columns allowed values: NYC Taxi Zone IDs range from 1 to 265\n",
    "\n",
    "# ===== PULocationID Validation =====\n",
    "print(\"=== PULocationID (Pickup Location) Validation ===\")\n",
    "print(f\"Rows before validation: {len(df):,}\")\n",
    "# Check for NaN values first\n",
    "nan_pu_count = df['PULocationID'].isna().sum()\n",
    "if nan_pu_count > 0:\n",
    "    print(f\"NaN/Missing PULocationID: {nan_pu_count:,} rows ({(nan_pu_count / len(df)) * 100:.2f}%)\")\n",
    "    # Drop rows with NaN PULocationID (cannot assume a default pickup location)\n",
    "    df = df[df['PULocationID'].notna()].copy()\n",
    "    print(f\"Dropped missing PULocationID entries.\\n\")\n",
    "\n",
    "# Create boolean mask for invalid PULocationIDs (outside range 1-265)\n",
    "invalid_pu = (df['PULocationID'] < 1) | (df['PULocationID'] > 265)\n",
    "invalid_pu_count = invalid_pu.sum()\n",
    "\n",
    "# Show top 10 most common valid pickup locations\n",
    "valid_pu = ~invalid_pu\n",
    "print(f\"Top 10 pickup locations:\")\n",
    "top_pu = df.loc[valid_pu, 'PULocationID'].value_counts().head(10)\n",
    "for location_id, count in top_pu.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  LocationID {int(location_id)}: {count:,} rows ({percentage:.2f}%)\")\n",
    "\n",
    "if invalid_pu_count > 0:\n",
    "    print(f\"\\n=== Invalid PULocationID Values ===\")\n",
    "    print(f\"Invalid PULocationID entries: {invalid_pu_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_pu_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Show invalid values (limited to first 20)\n",
    "    invalid_pu_values = df.loc[invalid_pu, 'PULocationID'].value_counts().sort_index().head(20)\n",
    "    print(f\"\\nInvalid PULocationID values found (showing up to 20):\")\n",
    "    for location_id, count in invalid_pu_values.items():\n",
    "        print(f\"  LocationID {location_id}: {count:,} rows\")\n",
    "    \n",
    "    # Drop invalid PULocationIDs\n",
    "    df = df[~invalid_pu].copy()\n",
    "    print(f\"\\nDropped invalid PULocationID entries.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "else:\n",
    "    print(\"\\nNo invalid PULocationID entries found.\")\n",
    "\n",
    "print(\"*******************************************\")\n",
    "\n",
    "# ===== DOLocationID Validation =====\n",
    "print(\"=== DOLocationID (Dropoff Location) Validation ===\")\n",
    "\n",
    "# Check for NaN values first\n",
    "nan_do_count = df['DOLocationID'].isna().sum()\n",
    "if nan_do_count > 0:\n",
    "    print(f\"NaN/Missing DOLocationID: {nan_do_count:,} rows ({(nan_do_count / len(df)) * 100:.2f}%)\")\n",
    "    # Drop rows with NaN DOLocationID (cannot assume a default dropoff location)\n",
    "    df = df[df['DOLocationID'].notna()].copy()\n",
    "    print(f\"Dropped missing DOLocationID entries.\\n\")\n",
    "\n",
    "# Create boolean mask for invalid DOLocationIDs (outside range 1-265)\n",
    "invalid_do = (df['DOLocationID'] < 1) | (df['DOLocationID'] > 265)\n",
    "invalid_do_count = invalid_do.sum()\n",
    "\n",
    "# Show top 10 most common valid dropoff locations\n",
    "valid_do = ~invalid_do\n",
    "print(f\"Top 10 dropoff locations:\")\n",
    "top_do = df.loc[valid_do, 'DOLocationID'].value_counts().head(10)\n",
    "for location_id, count in top_do.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  LocationID {int(location_id)}: {count:,} rows ({percentage:.2f}%)\")\n",
    "\n",
    "if invalid_do_count > 0:\n",
    "    print(f\"\\n=== Invalid DOLocationID Values ===\")\n",
    "    print(f\"Invalid DOLocationID entries: {invalid_do_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_do_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Show invalid values (limited to first 20)\n",
    "    invalid_do_values = df.loc[invalid_do, 'DOLocationID'].value_counts().sort_index().head(20)\n",
    "    print(f\"\\nInvalid DOLocationID values found (showing up to 20):\")\n",
    "    for location_id, count in invalid_do_values.items():\n",
    "        print(f\"  LocationID {location_id}: {count:,} rows\")\n",
    "    \n",
    "    # Drop invalid DOLocationIDs\n",
    "    df = df[~invalid_do].copy()\n",
    "    print(f\"\\nDropped invalid DOLocationID entries.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "else:\n",
    "    print(\"\\nNo invalid DOLocationID entries found.\")\n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb175868",
   "metadata": {},
   "source": [
    "payment_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2562e627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Payment Type Overview (Before Filtering) ===\n",
      "Rows before validation: 3,458,114\n",
      "\n",
      "All payment_type distribution:\n",
      "  Type 0 (Flex Fare trip): 741,846 rows (21.45%) [✓ Valid]\n",
      "  Type 1 (Credit card): 2,307,575 rows (66.73%) [✓ Valid]\n",
      "  Type 2 (Cash): 327,262 rows (9.46%) [✓ Valid]\n",
      "  Type 3 (No charge): 16,079 rows (0.46%) [✗ To be dropped]\n",
      "  Type 4 (Dispute): 65,352 rows (1.89%) [✗ To be dropped]\n",
      "\n",
      "=== Invalid/Unwanted Payment Types ===\n",
      "Payment types to drop (3, 4, 5, 6): 81,431\n",
      "Percentage of entries to drop: 2.28%\n",
      "\n",
      "Payment types being dropped:\n",
      "  Type 3 (No charge): 16,079 rows (0.46%)\n",
      "  Type 4 (Dispute): 65,352 rows (1.89%)\n",
      "\n",
      "Dropped payment types 3, 4, 5, 6.\n",
      "Rows remaining: 3,376,683\n",
      "\n",
      "=== Payment Type Overview (After Filtering) ===\n",
      "  Type 0 (Flex Fare trip): 741,846 rows (21.97%)\n",
      "  Type 1 (Credit card): 2,307,575 rows (68.34%)\n",
      "  Type 2 (Cash): 327,262 rows (9.69%)\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Allowed payment_type values (only normal transactions: Flex Fare, Credit card, Cash)\n",
    "allowed_payment_types = [0, 1, 2]\n",
    "\n",
    "# Check for NaN values first and set to default\n",
    "nan_count = df['payment_type'].isna().sum()\n",
    "if nan_count > 0:\n",
    "    print(\"=== Payment Type Missing Values ===\")\n",
    "    print(f\"NaN/Missing payment_type: {nan_count:,} rows ({(nan_count / len(df)) * 100:.2f}%)\")\n",
    "    df['payment_type'] = df['payment_type'].fillna(1)\n",
    "    print(f\"Set missing payment_type values to default of 1 (Credit card).\\n\")\n",
    "\n",
    "# Create boolean mask for invalid payment_types (after filling NaN)\n",
    "invalid_payment = ~df['payment_type'].isin(allowed_payment_types)\n",
    "invalid_count = invalid_payment.sum()\n",
    "\n",
    "# Show overview of all payment_type values before filtering\n",
    "print(\"=== Payment Type Overview (Before Filtering) ===\")\n",
    "print(f\"Rows before validation: {len(df):,}\")\n",
    "\n",
    "print(f\"\\nAll payment_type distribution:\")\n",
    "all_values = df['payment_type'].value_counts().sort_index()\n",
    "for payment_type, count in all_values.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    # Payment type descriptions\n",
    "    payment_desc = {\n",
    "        0: \"Flex Fare trip\",\n",
    "        1: \"Credit card\",\n",
    "        2: \"Cash\",\n",
    "        3: \"No charge\",\n",
    "        4: \"Dispute\",\n",
    "        5: \"Unknown\",\n",
    "        6: \"Voided trip\"\n",
    "    }\n",
    "    desc = payment_desc.get(int(payment_type), \"Unknown\")\n",
    "    status = \"✓ Valid\" if int(payment_type) in allowed_payment_types else \"✗ To be dropped\"\n",
    "    print(f\"  Type {int(payment_type)} ({desc}): {count:,} rows ({percentage:.2f}%) [{status}]\")\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(f\"\\n=== Invalid/Unwanted Payment Types ===\")\n",
    "    print(f\"Payment types to drop (3, 4, 5, 6): {invalid_count:,}\")\n",
    "    print(f\"Percentage of entries to drop: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Show which payment types are being dropped\n",
    "    invalid_values = df.loc[invalid_payment, 'payment_type'].value_counts().sort_index()\n",
    "    print(f\"\\nPayment types being dropped:\")\n",
    "    for payment_type, count in invalid_values.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        payment_desc = {\n",
    "            3: \"No charge\",\n",
    "            4: \"Dispute\",\n",
    "            5: \"Unknown\",\n",
    "            6: \"Voided trip\"\n",
    "        }\n",
    "        desc = payment_desc.get(int(payment_type), \"Unknown\")\n",
    "        print(f\"  Type {int(payment_type)} ({desc}): {count:,} rows ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Drop invalid/unwanted payment_types\n",
    "    df = df[~invalid_payment].copy()\n",
    "    \n",
    "    print(f\"\\nDropped payment types 3, 4, 5, 6.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "    \n",
    "    # Show final distribution\n",
    "    print(f\"\\n=== Payment Type Overview (After Filtering) ===\")\n",
    "    final_values = df['payment_type'].value_counts().sort_index()\n",
    "    for payment_type, count in final_values.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        payment_desc = {\n",
    "            0: \"Flex Fare trip\",\n",
    "            1: \"Credit card\",\n",
    "            2: \"Cash\"\n",
    "        }\n",
    "        desc = payment_desc.get(int(payment_type), \"Unknown\")\n",
    "        print(f\"  Type {int(payment_type)} ({desc}): {count:,} rows ({percentage:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo invalid/unwanted payment types found.\")\n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5a7f2",
   "metadata": {},
   "source": [
    "fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7984f270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fare Amount Statistics (Before Filtering) ===\n",
      "Rows before validation: 3,376,683\n",
      "Mean fare: $17.06\n",
      "Median fare: $12.80\n",
      "Min fare: $-653.30\n",
      "Max fare: $1173.40\n",
      "Std deviation: $15.75\n",
      "\n",
      "=== Converting Negative Fare Amounts ===\n",
      "Negative fare_amount entries: 126,600\n",
      "Converted 126,600 negative values to positive.\n",
      "\n",
      "=== Fare Amount Validation Report ===\n",
      "NaN fare_amount entries: 0\n",
      "Zero fare_amount entries: 448\n",
      "Fare_amount > $10,000 entries: 0\n",
      "\n",
      "Total invalid fare_amount entries: 448\n",
      "Percentage of invalid entries: 0.01%\n",
      "\n",
      "Dropped invalid fare_amount entries.\n",
      "Rows remaining: 3,376,235\n",
      "\n",
      "=== Fare Amount Statistics (After Filtering) ===\n",
      "Mean fare: $17.51\n",
      "Median fare: $12.80\n",
      "Min fare: $0.01\n",
      "Max fare: $1173.40\n",
      "Std deviation: $15.24\n",
      "\n",
      "Top 10 highest fare_amount entries:\n",
      "  $1173.40\n",
      "  $773.00\n",
      "  $724.00\n",
      "  $700.00\n",
      "  $682.70\n",
      "  $653.30\n",
      "  $653.30\n",
      "  $615.50\n",
      "  $607.80\n",
      "  $600.00\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Invalid fare_amount: NaN, zero, or > $10,000\n",
    "# Negative values will be converted to positive\n",
    "# Create boolean masks for each validation check\n",
    "is_nan = df['fare_amount'].isna()\n",
    "is_zero = df['fare_amount'] == 0\n",
    "is_negative = df['fare_amount'] < 0\n",
    "is_too_high = df['fare_amount'] > 10000\n",
    "\n",
    "# Show fare statistics BEFORE filtering\n",
    "print(\"=== Fare Amount Statistics (Before Filtering) ===\")\n",
    "print(f\"Rows before validation: {len(df):,}\")\n",
    "print(f\"Mean fare: ${df['fare_amount'].mean():.2f}\")\n",
    "print(f\"Median fare: ${df['fare_amount'].median():.2f}\")\n",
    "print(f\"Min fare: ${df['fare_amount'].min():.2f}\")\n",
    "print(f\"Max fare: ${df['fare_amount'].max():.2f}\")\n",
    "print(f\"Std deviation: ${df['fare_amount'].std():.2f}\")\n",
    "\n",
    "# Convert negative values to positive\n",
    "negative_count = is_negative.sum()\n",
    "if negative_count > 0:\n",
    "    print(f\"\\n=== Converting Negative Fare Amounts ===\")\n",
    "    print(f\"Negative fare_amount entries: {negative_count:,}\")\n",
    "    df.loc[is_negative, 'fare_amount'] = df.loc[is_negative, 'fare_amount'].abs()\n",
    "    print(f\"Converted {negative_count:,} negative values to positive.\\n\")\n",
    "\n",
    "# Combine invalid conditions (excluding negative since we fixed them)\n",
    "all_invalid = is_nan | is_zero | is_too_high\n",
    "invalid_count = all_invalid.sum()\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(f\"=== Fare Amount Validation Report ===\")\n",
    "    print(f\"NaN fare_amount entries: {is_nan.sum():,}\")\n",
    "    print(f\"Zero fare_amount entries: {is_zero.sum():,}\")\n",
    "    print(f\"Fare_amount > $10,000 entries: {is_too_high.sum():,}\")\n",
    "    print(f\"\\nTotal invalid fare_amount entries: {invalid_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Drop invalid fare_amount entries\n",
    "    df = df[~all_invalid].copy()\n",
    "    \n",
    "    print(f\"\\nDropped invalid fare_amount entries.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "    \n",
    "    # Show fare statistics AFTER filtering\n",
    "    print(f\"\\n=== Fare Amount Statistics (After Filtering) ===\")\n",
    "    print(f\"Mean fare: ${df['fare_amount'].mean():.2f}\")\n",
    "    print(f\"Median fare: ${df['fare_amount'].median():.2f}\")\n",
    "    print(f\"Min fare: ${df['fare_amount'].min():.2f}\")\n",
    "    print(f\"Max fare: ${df['fare_amount'].max():.2f}\")\n",
    "    print(f\"Std deviation: ${df['fare_amount'].std():.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo invalid fare_amount entries found.\")\n",
    "    \n",
    "    # Show fare statistics AFTER conversion\n",
    "    print(f\"\\n=== Fare Amount Statistics (After Conversion) ===\")\n",
    "    print(f\"Mean fare: ${df['fare_amount'].mean():.2f}\")\n",
    "    print(f\"Median fare: ${df['fare_amount'].median():.2f}\")\n",
    "    print(f\"Min fare: ${df['fare_amount'].min():.2f}\")\n",
    "    print(f\"Max fare: ${df['fare_amount'].max():.2f}\")\n",
    "    print(f\"Std deviation: ${df['fare_amount'].std():.2f}\")\n",
    "\n",
    "# print highest fare_amount entries\n",
    "highest_fares = df['fare_amount'].sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 highest fare_amount entries:\")\n",
    "for fare in highest_fares:\n",
    "    print(f\"  ${fare:.2f}\") \n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af272244",
   "metadata": {},
   "source": [
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77023108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Extra Amount Statistics (Before Filtering) ===\n",
      "Rows before validation: 3,376,235\n",
      "Mean extra: $1.29\n",
      "Median extra: $0.00\n",
      "Min extra: $-7.50\n",
      "Max extra: $13.50\n",
      "Std deviation: $1.84\n",
      "\n",
      "=== Converting Negative Extra Amounts ===\n",
      "Negative extra entries: 5,998\n",
      "Converted 5,998 negative values to positive.\n",
      "\n",
      "\n",
      "No invalid extra entries found.\n",
      "\n",
      "=== Extra Amount Statistics (After Conversion) ===\n",
      "Mean extra: $1.30\n",
      "Median extra: $0.00\n",
      "Min extra: $0.00\n",
      "Max extra: $13.50\n",
      "Std deviation: $1.83\n",
      "\n",
      "Top 10 highest extra entries:\n",
      "  $13.50\n",
      "  $13.25\n",
      "  $13.25\n",
      "  $13.25\n",
      "  $12.50\n",
      "  $12.50\n",
      "  $12.50\n",
      "  $12.50\n",
      "  $12.50\n",
      "  $12.50\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Invalid extra: NaN, zero, or > $10,000\n",
    "# Negative values will be converted to positive\n",
    "# Create boolean masks for each validation check\n",
    "is_nan = df['extra'].isna()\n",
    "is_zero = df['extra'] == 0\n",
    "is_negative = df['extra'] < 0\n",
    "is_too_high = df['extra'] > 10000\n",
    "\n",
    "# Show extra statistics BEFORE filtering\n",
    "print(\"=== Extra Amount Statistics (Before Filtering) ===\")\n",
    "print(f\"Rows before validation: {len(df):,}\")\n",
    "print(f\"Mean extra: ${df['extra'].mean():.2f}\")\n",
    "print(f\"Median extra: ${df['extra'].median():.2f}\")\n",
    "print(f\"Min extra: ${df['extra'].min():.2f}\")\n",
    "print(f\"Max extra: ${df['extra'].max():.2f}\")\n",
    "print(f\"Std deviation: ${df['extra'].std():.2f}\")\n",
    "\n",
    "# Convert negative values to positive\n",
    "negative_count = is_negative.sum()\n",
    "if negative_count > 0:\n",
    "    print(f\"\\n=== Converting Negative Extra Amounts ===\")\n",
    "    print(f\"Negative extra entries: {negative_count:,}\")\n",
    "    df.loc[is_negative, 'extra'] = df.loc[is_negative, 'extra'].abs()\n",
    "    print(f\"Converted {negative_count:,} negative values to positive.\\n\")\n",
    "\n",
    "# Combine invalid conditions (excluding negative since we fixed them)\n",
    "all_invalid = is_nan | is_too_high\n",
    "invalid_count = all_invalid.sum()\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(f\"=== Extra Amount Validation Report ===\")\n",
    "    print(f\"NaN extra entries: {is_nan.sum():,}\")\n",
    "    print(f\"Zero extra entries: {is_zero.sum():,}\")\n",
    "    print(f\"extra > $10,000 entries: {is_too_high.sum():,}\")\n",
    "    print(f\"\\nTotal invalid extra entries: {invalid_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Drop invalid fare_amount entries\n",
    "    df = df[~all_invalid].copy()\n",
    "    \n",
    "    print(f\"\\nDropped invalid extra entries.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "    \n",
    "    # Show fare statistics AFTER filtering\n",
    "    print(f\"\\n=== Extra Amount Statistics (After Filtering) ===\")\n",
    "    print(f\"Mean extra: ${df['extra'].mean():.2f}\")\n",
    "    print(f\"Median extra: ${df['extra'].median():.2f}\")\n",
    "    print(f\"Min extra: ${df['extra'].min():.2f}\")\n",
    "    print(f\"Max extra: ${df['extra'].max():.2f}\")\n",
    "    print(f\"Std deviation: ${df['extra'].std():.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo invalid extra entries found.\")\n",
    "    \n",
    "    # Show fare statistics AFTER conversion\n",
    "    print(f\"\\n=== Extra Amount Statistics (After Conversion) ===\")\n",
    "    print(f\"Mean extra: ${df['extra'].mean():.2f}\")\n",
    "    print(f\"Median extra: ${df['extra'].median():.2f}\")\n",
    "    print(f\"Min extra: ${df['extra'].min():.2f}\")\n",
    "    print(f\"Max extra: ${df['extra'].max():.2f}\")\n",
    "    print(f\"Std deviation: ${df['extra'].std():.2f}\")\n",
    "\n",
    "# print highest extra entries\n",
    "highest_extras = df['extra'].sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 highest extra entries:\")\n",
    "for extra in highest_extras:\n",
    "    print(f\"  ${extra:.2f}\") \n",
    "\n",
    "print(\"*******************************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbf6e5",
   "metadata": {},
   "source": [
    "mta_tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88756742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Unique mta_tax Values (Before Filtering) ===\n",
      "  $-0.50: 11,389 rows\n",
      "  $0.00: 18,573 rows\n",
      "  $0.50: 3,346,212 rows\n",
      "  $1.00: 52 rows\n",
      "  $1.75: 1 rows\n",
      "  $2.25: 1 rows\n",
      "  $4.00: 1 rows\n",
      "  $4.75: 4 rows\n",
      "  $10.50: 2 rows\n",
      "*******************************************\n",
      "=== mta_tax Amount Statistics (Before Filtering) ===\n",
      "Rows before validation: 3,376,235\n",
      "Mean mta_tax: $0.49\n",
      "Median mta_tax: $0.50\n",
      "Min mta_tax: $-0.50\n",
      "Max mta_tax: $10.50\n",
      "Std deviation: $0.07\n",
      "\n",
      "=== Converting Negative mta_tax Amounts ===\n",
      "Negative mta_tax entries: 11,389\n",
      "Converted 11,389 negative values to positive.\n",
      "\n",
      "=== mta_tax Amount Validation Report ===\n",
      "NaN mta_tax entries: 0\n",
      "Zero mta_tax entries: 1,799,248\n",
      "mta_tax != $0.50 entries: -3,364,786\n",
      "\n",
      "Total invalid mta_tax entries: 11,450\n",
      "Percentage of invalid entries: 0.32%\n",
      "\n",
      "Dropped invalid mta_tax entries.\n",
      "Rows remaining: 3,364,785\n",
      "\n",
      "=== mta_tax Amount Statistics (After Filtering) ===\n",
      "Mean mta_tax: $0.50\n",
      "Median mta_tax: $0.50\n",
      "Min mta_tax: $0.00\n",
      "Max mta_tax: $0.50\n",
      "Std deviation: $0.04\n",
      "\n",
      "Top 10 highest mta_tax entries:\n",
      "  $0.50\n",
      "  $0.50\n",
      "  $0.50\n",
      "  $0.50\n",
      "  $0.50\n",
      "  $0.50\n",
      "  $0.50\n",
      "  $0.50\n",
      "  $0.50\n",
      "  $0.50\n",
      "\n",
      "=== mta_tax Value Counts ===\n",
      "  mta_tax $0.00: 18,573 rows\n",
      "  mta_tax $0.50: 3,346,212 rows\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Mobilitätssteuer für den städtischen Pendlerverkehr, 0,50 $ pro Fahrt in NYC für bestimmte dropoff-Ziele.\n",
    "# Validierung der dropoff-Ziele wird ignoriert!\n",
    "# Invalid mta_tax: NaN, zero, or > $10,000\n",
    "# Negative values will be converted to positive\n",
    "# Create boolean masks for each validation check\n",
    "is_nan = df['mta_tax'].isna()\n",
    "# valid mta_tax is exactly 0.5 or zero\n",
    "is_valid = df['mta_tax'].isin([0, 0.5])\n",
    "is_negative = df['mta_tax'] < 0\n",
    "\n",
    "# print all unique mta_tax values before filtering with counts\n",
    "unique_mta_taxes = df['mta_tax'].unique()\n",
    "print(\"=== Unique mta_tax Values (Before Filtering) ===\")\n",
    "for mta_tax in sorted(unique_mta_taxes):\n",
    "    print(f\"  ${mta_tax:.2f}: {df[df['mta_tax'] == mta_tax].shape[0]:,} rows\")\n",
    "print(\"*******************************************\")\n",
    "\n",
    "# Show mta_tax statistics BEFORE filtering\n",
    "print(\"=== mta_tax Amount Statistics (Before Filtering) ===\")\n",
    "print(f\"Rows before validation: {len(df):,}\")\n",
    "print(f\"Mean mta_tax: ${df['mta_tax'].mean():.2f}\")\n",
    "print(f\"Median mta_tax: ${df['mta_tax'].median():.2f}\")\n",
    "print(f\"Min mta_tax: ${df['mta_tax'].min():.2f}\")\n",
    "print(f\"Max mta_tax: ${df['mta_tax'].max():.2f}\")\n",
    "print(f\"Std deviation: ${df['mta_tax'].std():.2f}\")\n",
    "\n",
    "# Convert negative values to positive\n",
    "negative_count = is_negative.sum()\n",
    "if negative_count > 0:\n",
    "    print(f\"\\n=== Converting Negative mta_tax Amounts ===\")\n",
    "    print(f\"Negative mta_tax entries: {negative_count:,}\")\n",
    "    df.loc[is_negative, 'mta_tax'] = df.loc[is_negative, 'mta_tax'].abs()\n",
    "    print(f\"Converted {negative_count:,} negative values to positive.\\n\")\n",
    "\n",
    "# Combine invalid conditions (excluding negative since we fixed them)\n",
    "all_invalid = is_nan | ~is_valid\n",
    "invalid_count = all_invalid.sum()\n",
    "\n",
    "if invalid_count > 0:\n",
    "    print(f\"=== mta_tax Amount Validation Report ===\")\n",
    "    print(f\"NaN mta_tax entries: {is_nan.sum():,}\")\n",
    "    print(f\"Zero mta_tax entries: {is_zero.sum():,}\")\n",
    "    print(f\"mta_tax != $0.50 entries: {~is_valid.sum():,}\")\n",
    "    print(f\"\\nTotal invalid mta_tax entries: {invalid_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Drop invalid mta_tax entries\n",
    "    df = df[~all_invalid].copy()\n",
    "    \n",
    "    print(f\"\\nDropped invalid mta_tax entries.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "    \n",
    "    # Show mta_tax statistics AFTER filtering\n",
    "    print(f\"\\n=== mta_tax Amount Statistics (After Filtering) ===\")\n",
    "    print(f\"Mean mta_tax: ${df['mta_tax'].mean():.2f}\")\n",
    "    print(f\"Median mta_tax: ${df['mta_tax'].median():.2f}\")\n",
    "    print(f\"Min mta_tax: ${df['mta_tax'].min():.2f}\")\n",
    "    print(f\"Max mta_tax: ${df['mta_tax'].max():.2f}\")\n",
    "    print(f\"Std deviation: ${df['mta_tax'].std():.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo invalid mta_tax entries found.\")\n",
    "    \n",
    "    # Show mta_tax statistics AFTER conversion\n",
    "    print(f\"\\n=== mta_tax Amount Statistics (After Conversion) ===\")\n",
    "    print(f\"Mean mta_tax: ${df['mta_tax'].mean():.2f}\")\n",
    "    print(f\"Median mta_tax: ${df['mta_tax'].median():.2f}\")\n",
    "    print(f\"Min mta_tax: ${df['mta_tax'].min():.2f}\")\n",
    "    print(f\"Max mta_tax: ${df['mta_tax'].max():.2f}\")\n",
    "    print(f\"Std deviation: ${df['mta_tax'].std():.2f}\")\n",
    "\n",
    "# print highest mta_tax entries\n",
    "highest_mta_taxes = df['mta_tax'].sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 highest mta_tax entries:\")\n",
    "for mta_tax in highest_mta_taxes:\n",
    "    print(f\"  ${mta_tax:.2f}\")\n",
    "\n",
    "\n",
    "# print numbers of values in mta_tax\n",
    "mta_tax_counts = df['mta_tax'].value_counts().sort_index()\n",
    "print(\"\\n=== mta_tax Value Counts ===\")\n",
    "for mta_tax, count in mta_tax_counts.items():\n",
    "    print(f\"  mta_tax ${mta_tax:.2f}: {count:,} rows\")  \n",
    "print(\"*******************************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211717be",
   "metadata": {},
   "source": [
    "tip_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a61016f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tip Amount Statistics (Before Filtering) ===\n",
      "Rows before validation: 3,364,785\n",
      "Mean tip: $2.85\n",
      "Median tip: $2.35\n",
      "Min tip: $0.00\n",
      "Max tip: $440.00\n",
      "Std deviation: $3.61\n",
      "\n",
      "=== Tip Amount Statistics (After Filtering) ===\n",
      "Mean tip: $2.85\n",
      "Median tip: $2.35\n",
      "Min tip: $0.00\n",
      "Max tip: $440.00\n",
      "Std deviation: $3.61\n",
      "\n",
      "=== Top 10 Highest Tips ===\n",
      "  1. $440.00\n",
      "  2. $225.00\n",
      "  3. $220.00\n",
      "  4. $180.30\n",
      "  5. $180.00\n",
      "  6. $171.07\n",
      "  7. $166.00\n",
      "  8. $150.00\n",
      "  9. $146.39\n",
      "  10. $128.00\n",
      "\n",
      "=== Tip Amount Distribution ===\n",
      "Trips with no tip ($0): 1,137,384 (33.80%)\n",
      "Trips with tips: 2,227,401 (66.20%)\n",
      "\n",
      "=== Tip Amount by Payment Type ===\n",
      "  Payment Type 0 (Flex Fare trip):\n",
      "    Total Tips: $237,960.41\n",
      "    Number of Trips: 741,772\n",
      "    Average Tip per Trip: $0.32\n",
      "  Payment Type 1 (Credit card):\n",
      "    Total Tips: $9,349,520.33\n",
      "    Number of Trips: 2,307,534\n",
      "    Average Tip per Trip: $4.05\n",
      "  Payment Type 2 (Cash):\n",
      "    Total Tips: $259.62\n",
      "    Number of Trips: 315,479\n",
      "    Average Tip per Trip: $0.00\n",
      "\n",
      "=== Cash Payment Tip Validation ===\n",
      "Cash payments with recorded tips (data error): 32\n",
      "Percentage: 0.00%\n",
      "Dropped cash trips with tips > $0.\n",
      "Rows remaining: 3,364,753\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Tip amount validation\n",
    "# Tips can be $0 (cash payments don't record tips) or positive amounts\n",
    "# Negative values will be converted to positive\n",
    "# NaN values will be set to 0\n",
    "# Extremely high tips (> $1000) will be dropped as outliers\n",
    "\n",
    "# Create boolean masks for each validation check\n",
    "is_nan = df['tip_amount'].isna()\n",
    "is_negative = df['tip_amount'] < 0\n",
    "is_too_high = df['tip_amount'] > 1000\n",
    "\n",
    "# Show tip_amount statistics BEFORE filtering\n",
    "print(\"=== Tip Amount Statistics (Before Filtering) ===\")\n",
    "print(f\"Rows before validation: {len(df):,}\")\n",
    "print(f\"Mean tip: ${df['tip_amount'].mean():.2f}\")\n",
    "print(f\"Median tip: ${df['tip_amount'].median():.2f}\")\n",
    "print(f\"Min tip: ${df['tip_amount'].min():.2f}\")\n",
    "print(f\"Max tip: ${df['tip_amount'].max():.2f}\")\n",
    "print(f\"Std deviation: ${df['tip_amount'].std():.2f}\")\n",
    "\n",
    "# Handle NaN values - set to 0 (no tip recorded)\n",
    "nan_count = is_nan.sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"\\n=== Handling Missing Tip Amounts ===\")\n",
    "    print(f\"NaN tip_amount entries: {nan_count:,}\")\n",
    "    df['tip_amount'] = df['tip_amount'].fillna(0)\n",
    "    print(f\"Set missing tip_amount values to $0.00.\")\n",
    "\n",
    "# Convert negative values to positive\n",
    "negative_count = is_negative.sum()\n",
    "if negative_count > 0:\n",
    "    print(f\"\\n=== Converting Negative Tip Amounts ===\")\n",
    "    print(f\"Negative tip_amount entries: {negative_count:,}\")\n",
    "    df.loc[is_negative, 'tip_amount'] = df.loc[is_negative, 'tip_amount'].abs()\n",
    "    print(f\"Converted {negative_count:,} negative values to positive.\")\n",
    "\n",
    "# Drop extremely high tips (> $1000)\n",
    "invalid_count = is_too_high.sum()\n",
    "if invalid_count > 0:\n",
    "    print(f\"\\n=== Tip Amount Validation Report ===\")\n",
    "    print(f\"Tip_amount > $1,000 entries: {invalid_count:,}\")\n",
    "    print(f\"Percentage of invalid entries: {(invalid_count / initial_row_count) * 100:.2f}%\")\n",
    "    \n",
    "    # Drop invalid tip_amount entries\n",
    "    df = df[~is_too_high].copy()\n",
    "    \n",
    "    print(f\"Dropped invalid tip_amount entries.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "\n",
    "# Show tip statistics AFTER filtering/conversion\n",
    "print(f\"\\n=== Tip Amount Statistics (After Filtering) ===\")\n",
    "print(f\"Mean tip: ${df['tip_amount'].mean():.2f}\")\n",
    "print(f\"Median tip: ${df['tip_amount'].median():.2f}\")\n",
    "print(f\"Min tip: ${df['tip_amount'].min():.2f}\")\n",
    "print(f\"Max tip: ${df['tip_amount'].max():.2f}\")\n",
    "print(f\"Std deviation: ${df['tip_amount'].std():.2f}\")\n",
    "\n",
    "# Show highest tip_amount entries\n",
    "highest_tips = df.nlargest(10, 'tip_amount')['tip_amount']\n",
    "print(\"\\n=== Top 10 Highest Tips ===\")\n",
    "for idx, tip in enumerate(highest_tips, 1):\n",
    "    print(f\"  {idx}. ${tip:.2f}\")\n",
    "\n",
    "# Show distribution of tip amounts\n",
    "print(\"\\n=== Tip Amount Distribution ===\")\n",
    "no_tip_count = (df['tip_amount'] == 0).sum()\n",
    "with_tip_count = (df['tip_amount'] > 0).sum()\n",
    "print(f\"Trips with no tip ($0): {no_tip_count:,} ({(no_tip_count / len(df)) * 100:.2f}%)\")\n",
    "print(f\"Trips with tips: {with_tip_count:,} ({(with_tip_count / len(df)) * 100:.2f}%)\")\n",
    "\n",
    "# Show tip amount per payment type\n",
    "print(\"\\n=== Tip Amount by Payment Type ===\")\n",
    "payment_type_desc = {\n",
    "    0: \"Flex Fare trip\",\n",
    "    1: \"Credit card\",\n",
    "    2: \"Cash\"\n",
    "}\n",
    "\n",
    "for payment_type in allowed_payment_types:\n",
    "    desc = payment_type_desc.get(payment_type, \"Unknown\")\n",
    "    payment_mask = df['payment_type'] == payment_type\n",
    "    total_tips = df.loc[payment_mask, 'tip_amount'].sum()\n",
    "    trip_count = payment_mask.sum()\n",
    "    avg_tip = df.loc[payment_mask, 'tip_amount'].mean()\n",
    "    \n",
    "    print(f\"  Payment Type {payment_type} ({desc}):\")\n",
    "    print(f\"    Total Tips: ${total_tips:,.2f}\")\n",
    "    print(f\"    Number of Trips: {trip_count:,}\")\n",
    "    print(f\"    Average Tip per Trip: ${avg_tip:.2f}\")\n",
    "\n",
    "# Drop rows with payment_type 2 (Cash) and tip_amount > 0\n",
    "# Cash tips are not recorded in the system, so this indicates a data error\n",
    "print(\"\\n=== Cash Payment Tip Validation ===\")\n",
    "cash_with_tips = (df['payment_type'] == 2) & (df['tip_amount'] > 0)\n",
    "cash_with_tips_count = cash_with_tips.sum()\n",
    "\n",
    "if cash_with_tips_count > 0:\n",
    "    print(f\"Cash payments with recorded tips (data error): {cash_with_tips_count:,}\")\n",
    "    print(f\"Percentage: {(cash_with_tips_count / len(df)) * 100:.2f}%\")\n",
    "    df = df[~cash_with_tips].copy()\n",
    "    print(f\"Dropped cash trips with tips > $0.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "else:\n",
    "    print(\"No cash payments with tips found (expected behavior).\")\n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02762eac",
   "metadata": {},
   "source": [
    "tolls_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "529dca68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tolls Amount Statistics (Before Filtering) ===\n",
      "Rows before validation: 3,364,753\n",
      "Mean tolls: $0.42\n",
      "Median tolls: $0.00\n",
      "Min tolls: $-48.18\n",
      "Max tolls: $115.87\n",
      "Std deviation: $1.87\n",
      "\n",
      "=== Converting Negative Tolls Amounts ===\n",
      "Negative tolls_amount entries: 123 (0.00%)\n",
      "Converted 123 negative values to positive.\n",
      "\n",
      "=== High Tolls Amount Review ===\n",
      "Tolls_amount > $100 entries: 3 (0.00%)\n",
      "\n",
      "High toll amounts found:\n",
      "  $101.94: 1 trips\n",
      "  $113.93: 1 trips\n",
      "  $115.87: 1 trips\n",
      "\n",
      "Dropped tolls > $100 as outliers.\n",
      "Rows remaining: 3,364,750\n",
      "\n",
      "=== Tolls Amount Statistics (After Filtering) ===\n",
      "Mean tolls: $0.42\n",
      "Median tolls: $0.00\n",
      "Min tolls: $0.00\n",
      "Max tolls: $92.06\n",
      "Std deviation: $1.87\n",
      "\n",
      "=== Tolls Amount Distribution ===\n",
      "Trips with no tolls ($0): 3,174,521 (94.35%)\n",
      "Trips with tolls: 190,229 (5.65%)\n",
      "\n",
      "=== Top 20 Most Common Toll Amounts (excluding $0) ===\n",
      "  1. $6.94: 174,912 trips (91.95% of trips with tolls)\n",
      "  2. $14.06: 2,395 trips (1.26% of trips with tolls)\n",
      "  3. $16.06: 1,816 trips (0.95% of trips with tolls)\n",
      "  4. $3.18: 1,669 trips (0.88% of trips with tolls)\n",
      "  5. $13.88: 1,121 trips (0.59% of trips with tolls)\n",
      "  6. $5.20: 517 trips (0.27% of trips with tolls)\n",
      "  7. $21.00: 374 trips (0.20% of trips with tolls)\n",
      "  8. $11.19: 352 trips (0.19% of trips with tolls)\n",
      "  9. $2.60: 301 trips (0.16% of trips with tolls)\n",
      "  10. $9.00: 292 trips (0.15% of trips with tolls)\n",
      "  11. $23.00: 279 trips (0.15% of trips with tolls)\n",
      "  12. $21.06: 190 trips (0.10% of trips with tolls)\n",
      "  13. $22.06: 189 trips (0.10% of trips with tolls)\n",
      "  14. $10.12: 181 trips (0.10% of trips with tolls)\n",
      "  15. $11.94: 181 trips (0.10% of trips with tolls)\n",
      "  16. $20.82: 181 trips (0.10% of trips with tolls)\n",
      "  17. $12.94: 165 trips (0.09% of trips with tolls)\n",
      "  18. $23.06: 160 trips (0.08% of trips with tolls)\n",
      "  19. $24.06: 158 trips (0.08% of trips with tolls)\n",
      "  20. $19.06: 155 trips (0.08% of trips with tolls)\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# Tolls amount validation\n",
    "# Tolls can be $0 (no tolls) or positive amounts\n",
    "# Negative values will be converted to positive\n",
    "# NaN values will be set to 0\n",
    "# Extremely high tolls (> $100) will be flagged and potentially dropped\n",
    "\n",
    "# Create boolean masks for each validation check\n",
    "is_nan = df['tolls_amount'].isna()\n",
    "is_negative = df['tolls_amount'] < 0\n",
    "is_too_high = df['tolls_amount'] > 100\n",
    "\n",
    "# Show tolls_amount statistics BEFORE filtering\n",
    "print(\"=== Tolls Amount Statistics (Before Filtering) ===\")\n",
    "print(f\"Rows before validation: {len(df):,}\")\n",
    "print(f\"Mean tolls: ${df['tolls_amount'].mean():.2f}\")\n",
    "print(f\"Median tolls: ${df['tolls_amount'].median():.2f}\")\n",
    "print(f\"Min tolls: ${df['tolls_amount'].min():.2f}\")\n",
    "print(f\"Max tolls: ${df['tolls_amount'].max():.2f}\")\n",
    "print(f\"Std deviation: ${df['tolls_amount'].std():.2f}\")\n",
    "\n",
    "# Handle NaN values - set to 0 (no tolls)\n",
    "nan_count = is_nan.sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"\\n=== Handling Missing Tolls Amounts ===\")\n",
    "    print(f\"NaN tolls_amount entries: {nan_count:,} ({(nan_count / len(df)) * 100:.2f}%)\")\n",
    "    df['tolls_amount'] = df['tolls_amount'].fillna(0)\n",
    "    print(f\"Set missing tolls_amount values to $0.00.\")\n",
    "\n",
    "# Convert negative values to positive\n",
    "negative_count = is_negative.sum()\n",
    "if negative_count > 0:\n",
    "    print(f\"\\n=== Converting Negative Tolls Amounts ===\")\n",
    "    print(f\"Negative tolls_amount entries: {negative_count:,} ({(negative_count / len(df)) * 100:.2f}%)\")\n",
    "    df.loc[is_negative, 'tolls_amount'] = df.loc[is_negative, 'tolls_amount'].abs()\n",
    "    print(f\"Converted {negative_count:,} negative values to positive.\")\n",
    "\n",
    "# Check for extremely high tolls (> $100)\n",
    "high_tolls_count = is_too_high.sum()\n",
    "if high_tolls_count > 0:\n",
    "    print(f\"\\n=== High Tolls Amount Review ===\")\n",
    "    print(f\"Tolls_amount > $100 entries: {high_tolls_count:,} ({(high_tolls_count / len(df)) * 100:.2f}%)\")\n",
    "    \n",
    "    # Show the high toll amounts\n",
    "    high_tolls = df[is_too_high]['tolls_amount'].value_counts().sort_index()\n",
    "    print(f\"\\nHigh toll amounts found:\")\n",
    "    for toll_amount, count in high_tolls.head(10).items():\n",
    "        print(f\"  ${toll_amount:.2f}: {count:,} trips\")\n",
    "    \n",
    "    # Drop extremely high tolls (likely data errors)\n",
    "    df = df[~is_too_high].copy()\n",
    "    print(f\"\\nDropped tolls > $100 as outliers.\")\n",
    "    print(f\"Rows remaining: {len(df):,}\")\n",
    "\n",
    "# Show tolls statistics AFTER filtering/conversion\n",
    "print(f\"\\n=== Tolls Amount Statistics (After Filtering) ===\")\n",
    "print(f\"Mean tolls: ${df['tolls_amount'].mean():.2f}\")\n",
    "print(f\"Median tolls: ${df['tolls_amount'].median():.2f}\")\n",
    "print(f\"Min tolls: ${df['tolls_amount'].min():.2f}\")\n",
    "print(f\"Max tolls: ${df['tolls_amount'].max():.2f}\")\n",
    "print(f\"Std deviation: ${df['tolls_amount'].std():.2f}\")\n",
    "\n",
    "# Show distribution of toll amounts\n",
    "print(f\"\\n=== Tolls Amount Distribution ===\")\n",
    "no_tolls = (df['tolls_amount'] == 0).sum()\n",
    "with_tolls = (df['tolls_amount'] > 0).sum()\n",
    "print(f\"Trips with no tolls ($0): {no_tolls:,} ({(no_tolls / len(df)) * 100:.2f}%)\")\n",
    "print(f\"Trips with tolls: {with_tolls:,} ({(with_tolls / len(df)) * 100:.2f}%)\")\n",
    "\n",
    "# Show top 20 most common toll amounts (excluding $0)\n",
    "if with_tolls > 0:\n",
    "    print(f\"\\n=== Top 20 Most Common Toll Amounts (excluding $0) ===\")\n",
    "    tolls_nonzero = df[df['tolls_amount'] > 0]['tolls_amount'].value_counts().head(20)\n",
    "    for idx, (toll_amount, count) in enumerate(tolls_nonzero.items(), 1):\n",
    "        percentage = (count / with_tolls) * 100\n",
    "        print(f\"  {idx}. ${toll_amount:.2f}: {count:,} trips ({percentage:.2f}% of trips with tolls)\")\n",
    "\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa5190",
   "metadata": {},
   "source": [
    "improvement_surcharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d850bc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN entries in improvement_surcharge: 0\n",
      "Number of negative improvement_surcharge entries: 285\n",
      "Value counts for improvement_surcharge:\n",
      "improvement_surcharge\n",
      " 1.0    3329740\n",
      " 0.0      34561\n",
      "-1.0        285\n",
      " 0.3        164\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print NaN from improvement_surcharge\n",
    "print(\"Number of NaN entries in improvement_surcharge:\", df['improvement_surcharge'].isna().sum())\n",
    "# print negative improvement_surcharge entries\n",
    "print(\"Number of negative improvement_surcharge entries:\", df[df['improvement_surcharge'] < 0].shape[0])   \n",
    "# print value counts \n",
    "print(\"Value counts for improvement_surcharge:\")\n",
    "print(df['improvement_surcharge'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af347a6b",
   "metadata": {},
   "source": [
    "total_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00f4c658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN entries in total_amount: 0\n",
      "Number of negative total_amount entries: 285\n"
     ]
    }
   ],
   "source": [
    "# print NaN from total_amount\n",
    "print(\"Number of NaN entries in total_amount:\", df['total_amount'].isna().sum())\n",
    "# print negative total_amount entries\n",
    "print(\"Number of negative total_amount entries:\", df[df['total_amount'] < 0].shape[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb6cda",
   "metadata": {},
   "source": [
    "congestion_surcharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c0136df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN entries in congestion_surcharge: 741772\n",
      "Number of negative congestion_surcharge entries: 99\n",
      "Value counts for congestion_surcharge:\n",
      "congestion_surcharge\n",
      " 2.5    2450684\n",
      " 0.0     172195\n",
      "-2.5         99\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print NaN from congestion_surcharge\n",
    "print(\"Number of NaN entries in congestion_surcharge:\", df['congestion_surcharge'].isna().sum())\n",
    "# print negative congestion_surcharge entries\n",
    "print(\"Number of negative congestion_surcharge entries:\", df[df['congestion_surcharge'] < 0].shape[0])\n",
    "# print value counts\n",
    "print(\"Value counts for congestion_surcharge:\")\n",
    "print(df['congestion_surcharge'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf12866",
   "metadata": {},
   "source": [
    "Airport_fee (field name case sensitive!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce1804bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN entries in airport_fee: 741772\n",
      "Number of negative airport_fee entries: 151\n",
      "Value counts for airport_fee:\n",
      "Airport_fee\n",
      " 0.00    2431913\n",
      " 1.75     190914\n",
      "-1.75        151\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print NaN from airport_fee\n",
    "print(\"Number of NaN entries in airport_fee:\", df['Airport_fee'].isna().sum())\n",
    "# print negative airport_fee entries\n",
    "print(\"Number of negative airport_fee entries:\", df[df['Airport_fee'] < 0].shape[0])\n",
    "# print value counts\n",
    "print(\"Value counts for airport_fee:\")\n",
    "print(df['Airport_fee'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e69ed",
   "metadata": {},
   "source": [
    "cbd_congestion_fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f115c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN entries in cbd_congestion_fee: 0\n",
      "Number of negative cbd_congestion_fee entries: 168\n"
     ]
    }
   ],
   "source": [
    "# print NaN from cbd_congestion_fee\n",
    "print(\"Number of NaN entries in cbd_congestion_fee:\", df['cbd_congestion_fee'].isna().sum())\n",
    "# print negative cbd_congestion_fee entries\n",
    "print(\"Number of negative cbd_congestion_fee entries:\", df[df['cbd_congestion_fee'] < 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b58d4",
   "metadata": {},
   "source": [
    "Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b7b3061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "percentage of duplicate rows before dropping duplicates:  0.0\n",
      "Number of rows in the dataframe after dropping duplicates: 3364750\n"
     ]
    }
   ],
   "source": [
    "# duplicate rows\n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum())\n",
    "print(\"percentage of duplicate rows before dropping duplicates: \", (df.duplicated().sum() / initial_row_count) * 100)\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Number of rows in the dataframe after dropping duplicates: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698fe38",
   "metadata": {},
   "source": [
    "CLEANING DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f32bc923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************\n",
      "***** COMPLETED CLEANING *****\n",
      "Initial Number of rows in the dataframe: 3,577,543\n",
      "Number of rows dropped: 212,793\n",
      "Final Number of rows in the dataframe: 3,364,750\n",
      "Percentage of rows dropped: 5.95%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"*******************************************\")\n",
    "print(\"***** COMPLETED CLEANING *****\")\n",
    "# dropped rows initial vs final \n",
    "print(f\"Initial Number of rows in the dataframe: {initial_row_count:,.0f}\")\n",
    "\n",
    "dropped_rows = initial_row_count - len(df)\n",
    "print(f\"Number of rows dropped: {dropped_rows:,.0f}\")\n",
    "\n",
    "print(f\"Final Number of rows in the dataframe: {len(df):,.0f}\")  \n",
    "\n",
    "# percentage of rows dropped\n",
    "percentage_dropped = (dropped_rows / initial_row_count) * 100\n",
    "print(f\"Percentage of rows dropped: {percentage_dropped:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893fc76",
   "metadata": {},
   "source": [
    "Tip per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8d913f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tip amount: $9,587,373.39\n",
      "Total passenger count: 4,102,387.0\n",
      "Average tip per passenger: $2.34\n"
     ]
    }
   ],
   "source": [
    "# tip per passenger count\n",
    "df['tip_per_passenger'] = df['tip_amount'] / df['passenger_count']\n",
    "#print(df.head(5))\n",
    "\n",
    "# print total tip_amount \n",
    "total_tip_amount = df['tip_amount'].sum()\n",
    "print(f\"Total tip amount: ${total_tip_amount:,.2f}\")\n",
    "\n",
    "# print total passenger count (formatting with commas)\n",
    "total_passenger_count = df['passenger_count'].sum()\n",
    "print(f\"Total passenger count: {total_passenger_count:,}\")\n",
    "# average tip per passenger\n",
    "average_tip_per_passenger = total_tip_amount / total_passenger_count\n",
    "print(f\"Average tip per passenger: ${average_tip_per_passenger:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f36ee02",
   "metadata": {},
   "source": [
    "Umsatz pro Weglänge (Verhältnis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24527d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print revenue per mile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab8598",
   "metadata": {},
   "source": [
    "Gesamtumsatz pro Taximeter (vendor_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "127d254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorName\n",
      "Creative Mobile Technologies, LLC    $18,713,698.43\n",
      "Curb Mobility, LLC                   $68,066,486.11\n",
      "Myle Technologies Inc                     $5,327.58\n",
      "Name: total_amount, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# mapping of VendorID to VendorName for better readability\n",
    "vendor_id_map = {1: 'Creative Mobile Technologies, LLC', 2: 'Curb Mobility, LLC', 6: 'Myle Technologies Inc', 7: 'Helix'}\n",
    "df['VendorName'] = df['VendorID'].map(vendor_id_map)\n",
    "# revenue per VendorName formatting in USD with 2 decimal places\n",
    "revenue_per_vendor = df.groupby('VendorName')['total_amount'].sum()\n",
    "revenue_per_vendor = revenue_per_vendor.apply(lambda x: f\"${x:,.2f}\")\n",
    "print(revenue_per_vendor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265fa92d",
   "metadata": {},
   "source": [
    "Median von Weglänge aller Fahrten pro Tag und Stunde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d3b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb231c9c",
   "metadata": {},
   "source": [
    "Visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa17d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
